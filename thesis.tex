\input{praeambel.tex} % Importiere die Einstellungen aus der Präambel
% hier beginnt der eigentliche Inhalt
\begin{document}
\pagenumbering{Roman} % Seitenummerierung mit großen römischen Zahlen 
\pagestyle{empty} % keine Kopf- oder Fußzeilen auf den ersten Seiten

% Titelseite
\clearscrheadings\clearscrplain
\begin{center}
\begin{Huge}
Themenspezifische Gruppierung deutscher Online-Zeitungen mit Natural Language Processing\\
\end{Huge}

\vspace{8mm}
Bachelorarbeit\\
\vspace{0.4cm}
\vspace{2 cm}
Georg Donner \\
Matrikel-Nummer 553821\\
\vspace{8cm}
\begin{tabular}{rl}
{\bfseries Betreuer} & Prof. Dr. Gefei Zhang\\
{\bfseries Erstprüfer} & Prof. Dr. Gefei Zhang\\
{\bfseries Zweitprüfer} & Prof. Dr. Barne Kleinen\\
\end{tabular}

\end{center}
\clearpage

\pagestyle{useheadings} % normale Kopf- und Fußzeilen für den Rest

\tableofcontents % erstelle hier das Inhaltsverzeichnis

% richtiger Inhalt
\pagenumbering{arabic} % ab jetzt die normale arabische Nummerierung
\chapter{Einleitung}
Natural Language Processing ist ein großes Feld, welches besonders in der letzten Zeit im Zuge der Digitalisierung viel an Aufmerksamkeit und Wichtigkeit gewonnen hat. Es ermöglicht uns Informationen schneller zu finden, Systeme durch gesprochene Sprache zu steuern oder ganze Texte zu generieren. Eine weitere Aufgabe ist es, eine große Menge an Texten in Kategorien einzuteilen, um die Daten auf eine gewünschte Teilmenge für eine spezifischere Suche oder Analyse zu reduzieren. Die Kategorisierung der Dokumente nach ihrem Inhalt ist hier der häufigste Anwendungsfall, es ist aber auch möglich Texte nach ihrem generellen Genre oder Schreibstil zu vergleichen.

Diese Arbeit wird am Beispiel deutscher Online-Zeitungen untersuchen, welche Möglichkeiten es gibt Texte unabhängig von ihrem Inhalt zu vergleichen. Dabei werden lexikalische, morphologische und syntaktische Merkmale, aber auch die Verwendung inhaltlich irrelevanter Wörter als Features verwendet. Es wird überprüft, inwiefern die Artikel gruppiert werden können und Rückschlüsse auf Unterschiede im Schreibstil ganzer Zeitungen statt nur einzelner Artikel zulassen.

Des Weiteren wird untersucht, ob und wie sich der Schreibstil einer Zeitung je nach Thema, wie z.B. Politik und Sport, unterscheidet.

\chapter{Grundlagen}
Wofür werden die folgenden Grundlagen beschrieben, warum relevant zum Thema?

\section{Natural Language Processing}
Teilgebiet der Informatik -> Künstliche Intelligenz und Maschinelles Lernen

Verarbeitung natürlicher Sprache
-> natürliche Sprache: Menschliche Sprachen, die sich über lange Zeit entwickelt haben

Große Herausforderung, da menschliche Sprachen sehr variabel sind, da sie sich in einem endlosen Entwicklungsprozess befinden. Die Analyse der Semantik eines Satzes oder Textes ist für Computer besonders schwierig, da sich die Bedeutung häufig erst durch den Kontext ergibt.

Mit den schnellen Fortschritten im Bereich des Maschinellen Lernens in den letzten Jahrzehnten, eröffneten sich für die Verarbeitung natürlicher Sprache völlig neue Möglichkeiten. Die Erkennung von Syntax und Semantik wurde damit immer präziser und das Teilgebiet immer relevanter. Jedoch basiert dies immer noch hauptsächlich auf Algorithmen des Supervised Learning für die die Texte vorher manuell annotiert werden müssen. Ein bekanntes Beispiel für einen Korpus deutscher Sprache mit solchen Annotationen ist der TIGER Corpus \footnote{http://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/tiger.html}.

\todo{Unterschied zwischen Englisch und Deutsch, welche Herausforderungen/Einschränkungen gibt es?}
\subsection{Pipeline}
Bei der Analyse eines Textes werden in der Regel verschiedene Schritte abgearbeitet, die jeweils eigene Merkmale der Sprache untersuchen. Es entsteht eine so genannte Pipeline, die je nach Anwendungsfall unterschiedlich aussieht. Das sequenzielle Ausführen dieser einzelnen Vorgänge ist notwendig, da beispielsweise die Analyse der Syntax voraussetzt, dass das Dokument bereits in Token zerlegt wurde. Im Folgenden werden die für diese Arbeit relevanten Schritte beschrieben.

\todo{eventuell noch andere Modelle beschreiben?}
\paragraph*{Tokenisierung}
\paragraph*{POS-Tagging}
\paragraph*{Dependency Parsing}
\paragraph*{Lemmatisierung}
\section{Machine Learning}
Was ist Machine Learning? Erklärung der Basics, die für diese Arbeit relevant sind.
\subsection{Feature Engineering}
Was bedeutet Feature Engineering? Sehr wichtiger Schritt; ausschlaggebend für das letztendliche Resultat. In Natural Language Processing sehr viele verschiedene Ansätze: teilweise Studien die sich nur damit befassen, welche Features es gibt/am besten geeignet sind um xy zu erkennen/erreichen. Anreißen welche Ansätze es gibt und welche in dieser Arbeit nicht berücksichtigt werden. Dieser Prozess wird Feature Selection genannt und ist sehr wichtig aus verschiedenen Gründen: Dimensionalität, Noise Reduction, kürzere Trainingszeiten (aus Wikipedia en).
\subsection{Dimensionsionalitätsreduktion}
Fluch der Dimensionalität: Je mehr Dimensionen es gibt, umso weniger sagt der Raum aus, da die Daten immer weiter voneinander entfernt liegen. Hängt davon ab wieviele Beobachtungen es gibt. Ist fürs Clustering ziemlich wichtig, weil dort die Distanzen wichtig sind. (es gibt auch andere Ansätze wie t-SNE, wo nur Cluster eine Aussagekraft haben und keine Entfernungen). Sehr häufig eingesetztes Verfahren: Hauptkomponentenanalyse (PCA). Features haben dann für sich betrachtet keine Aussagekraft mehr, aber ähnlicher Anteil an Informationen/Varianz bleibt erhalten, obwohl weniger Features. Gut dafür geeignet, Cluster in einem Datensatz zu erkennen. Bei Reduktion auf 2 oder 3 Features kann es dann auch sinnvoll geplottet werden. Dabei gehen aber oft viel zu viele Informationen verloren.
\subsection{Klassifizierung}
Was ist Klassifizierung? Zuordnung einer neuen Beobachtung zu einer Kategorie aus einem vordefinierten Set. Wurde vorher auf Basis eines Trainingssets trainiert, bei denen die Kategorie bekannt ist (supervised learning) (aus Wikipedia en). Wie funktioniert Klassifizierung grob (One vs All, Multinomial...). Welche Klassifizierungsalgorithmen gibt es? Auf jeden Fall keinen besten.

\chapter{Textverarbeitung}
Welche Schritte sind erforderlich, was muss besonders berücksichtigt werden?
\section{Verwendete Tools}
\subsection{Python}
Python ist die am häufigsten für Machine Learning verwendete Sprache und es gibt eine Vielzahl an packages die für nlp/ml optimiert sind. Welche packages wurden für diese Arbeit primär genutzt und warum so gut? Zwei sehr wichtige im Detail, sonst noch Numpy, Pandas und Matplotlib? Anaconda?
\subsection{SpaCy}
Was kann Spacy alles? Warum für Spacy entschieden? (ist für die deutsche Sprache schon sehr ausgereift). Vergleich zu anderen NLP tools und Überblick über die tatsächliche Schnelligkeit auf meiner Maschine beim processen?
\subsection{Scikit-learn}
Was kann sklearn alles? Warum für sklearn entschieden? Wie funktioniert es genau? Vergleiche zu anderen ML tools?
\section{Datenselektion}
Die Auswahl der Daten ist Grundlage für alles, blabla
\subsection{Datensatz}
Woher kommt der Datensatz? Wie groß? Welche Zeitungen? Überblick der Zeitspanne. Illustration wie verschieden die Artikel sind?
\subsection{Normalisierung}
Welche Normalisierungen mussten vor der Aufbereitung durchgeführt werden: Herausfiltern von viel zu kurzen Artikeln/Artikeln ohne Inhalt, Zuordnung von Kategorien zu jedem Artikel, Vereinheitlichung des Formats des Datums (ISO). Oft veröffentlichen Zeitungen auch Artikel einer Newsagentur wie z.B. der dpa, diese müssen herausgefiltert werden, um den Schreibstil einer Zeitung ermitteln zu können (nur Artikel von Autoren der Zeitung sollen berücksichtigt werden). Illustration zum Anteil der dpa Artikel.
\section{Textaufbereitung}
Analyse der Texten mit einem Natural Language Processing Tool, das die zuvor beschriebene Pipeline durchläuft (SpaCy). Vorherige Ansätze?
Nachbereitung der Texte: Entfernen von vermeintlichen Sätzen mit weniger als 4 Wörtern. Musste dort sehr rigoros sein, da jeglicher Inhalt mit enthalten war, so auch Kürzel wie (dpa), der Ort, Autor, Quellen oder Verweise.
\section{Featuregenerierung}
Es gibt eine Vielzahl an Features, die man für einen Text generieren bzw. auswählen kann, ohne dass dabei der Inhalt des Artikels einen Einfluss hat. Verweis zu Studien, die analysiert haben, welche Kombination an Features die besten Resultate liefern.
\subsection{Token-basiert}
Durchschnittliche Satzlänge, durchschnittliche Wortlänge?
Frequenz der n häufigsten Wörter (bei mir 30, 50, 100). Sind dann Wörter wie ['der', 'ich', 'dieser']
\subsection{Lexikalisch}
Type-token ratio: Beurteilt die Reichhaltigkeit des Vokabulars, dafür gibt es noch mehr präzisere Indizes (in der Studie von 2000 sind einige Beispiele). Nachteil: funktioniert besser, je länger die Texte sind und die Artikel sind durchschnittlich nicht besonders lang. Illustration dazu?
Readability: Wieder gibt es hier viele verschiedene Indizes. In dieser Arbeit verwendet wurde der Flesch-Reading Ease Index unter Berücksichtigung dass die Sprache Deutsch ist (gibt noch viel mehr für Englisch). Erforderte die Berechnung, wieviele Silben ein Wort hat (war in der Pipeline nicht mit drin)
\subsection{Morphologisch}
POS unigrams: Die Frequenz der einzelnen Wortarten für jeden Artikel, ist teilweise nur sehr gering und auch hier erhöht sich die Aussagekraft mit der Länge des Artikels.
Lexical Density: Wieviele Wörter des Artikels tragen zum Inhalt bei (Nomen, Verben, Adjektive, Adverbien) im Verhältnis zur Gesamtanzahl an Wörtern? Ähnlich zur Stopword Frequency, die auch berechnet wurde, aber zu sehr mit der lexikalischen Dichte korreliert.
\subsection{Syntaktisch}
Dependency relation unigrams: Die Frequenz der einzelnen Dependency relations für jeden Artikel, ist teilweise nur sehr gering und auch hier erhöht sich die Aussagekraft mit der Länge des Artikels.

\chapter{Datenauswertung}
\section{Auswertungsfunktionen}
\section{Versuche}
\subsection{Feature Selection}
\subsection{Feature Extraction}
\subsection{Clustering}
\section{Ergebnisse}

\chapter{Ergebnis}

\bibliographystyle{alphadin_martin}
\bibliography{bibliographie}
\listoffigures % erstelle hier das Abbildungsverzeichnis

\chapter*{Erklärung}

Hiermit versichere ich, dass ich die vorliegende Arbeit selbstständig verfasst und keine anderen als die angegebenen Quellen und Hilfsmittel benutzt habe, dass alle Stellen der Arbeit, die wörtlich oder sinngemäß aus anderen Quellen übernommen wurden, als solche kenntlich gemacht und dass die Arbeit in gleicher oder ähnlicher Form noch keiner Prüfungsbehörde vorgelegt wurde.

\vspace{3cm}
Ort, Datum \hspace{5cm} Unterschrift\\

\end{document}